# CMX-Enhanced MDX23C Configuration for Vocals Separation
# This configuration enables Channel Multiplexing for memory-efficient vocals separation
# Based on MARS paper (arXiv:2509.26007) - reduces memory by up to 75%

audio:
  chunk_size: 261120
  dim_f: 4096
  dim_t: 256
  hop_length: 1024
  n_fft: 8192
  num_channels: 2
  sample_rate: 44100
  min_mean_abs: 0.001
  
  # CMX (Channel Multiplexing) Configuration
  # Reduces spatial dimensions while preserving frequency information
  use_cmx: true           # Enable Channel Multiplexing
  cmx_reduction: 2        # Reduction factor: 2 = 75% spatial reduction
                          # Higher values = more memory savings
                          # Recommended: 2 (balanced), 3 (aggressive), 1 or false (disabled)

model:
  act: gelu
  bottleneck_factor: 4
  growth: 128
  norm: InstanceNorm
  num_blocks_per_scale: 2
  num_channels: 128       # Larger channel count works well with CMX
  num_scales: 5
  num_subbands: 4
  scale:
  - 2
  - 2

training:
  batch_size: 12          # Increased from 6 to 12 due to CMX memory savings (2x increase)
  gradient_accumulation_steps: 1
  grad_clip: 0
  instruments:
  - vocals
  - other
  lr: 0.000127            # Slightly increased from 9.0e-05 (~1.4x for larger batch)
  patience: 2
  reduce_factor: 0.95
  target_instrument: null
  num_epochs: 1000
  num_steps: 1000
  q: 0.95
  coarse_loss_clip: true
  ema_momentum: 0.999
  optimizer: adam
  read_metadata_procs: 8  # Number of processes to use during metadata reading for dataset
  other_fix: true         # Needed for checking on multisong dataset if other is actually instrumental
  use_amp: true           # Enable mixed precision (float16) - works great with CMX

augmentations:
  enable: true            # Enable all augmentations
  loudness: true          # Randomly change loudness of each stem
  loudness_min: 0.5
  loudness_max: 1.5
  mixup: true             # Mix several stems of same type
  mixup_probs: !!python/tuple
    - 0.2
    - 0.02
  mixup_loudness_min: 0.5
  mixup_loudness_max: 1.5

  # Apply mp3 compression to mixture only (emulate downloading mp3 from internet)
  mp3_compression_on_mixture: 0.01
  mp3_compression_on_mixture_bitrate_min: 32
  mp3_compression_on_mixture_bitrate_max: 320
  mp3_compression_on_mixture_backend: "lameenc"

  all:
    channel_shuffle: 0.5
    random_inverse: 0.1
    random_polarity: 0.5
    mp3_compression: 0.01
    mp3_compression_min_bitrate: 32
    mp3_compression_max_bitrate: 320
    mp3_compression_backend: "lameenc"

  vocals:
    pitch_shift: 0.1
    pitch_shift_min_semitones: -5
    pitch_shift_max_semitones: 5
    seven_band_parametric_eq: 0.25
    seven_band_parametric_eq_min_gain_db: -9
    seven_band_parametric_eq_max_gain_db: 9
    tanh_distortion: 0.1
    tanh_distortion_min: 0.1
    tanh_distortion_max: 0.7
  other:
    pitch_shift: 0.1
    pitch_shift_min_semitones: -4
    pitch_shift_max_semitones: 4
    gaussian_noise: 0.1
    gaussian_noise_min_amplitude: 0.001
    gaussian_noise_max_amplitude: 0.015
    time_stretch: 0.01
    time_stretch_min_rate: 0.8
    time_stretch_max_rate: 1.25

inference:
  batch_size: 1
  dim_t: 256
  num_overlap: 4

# CMX-Specific Notes for Vocals Separation:
# 
# 1. MEMORY SAVINGS:
#    - With cmx_reduction=2, spatial dimensions reduced by 75%
#    - Batch size increased from 6 to 12 (2x) on same GPU
#    - High-res spectrograms (n_fft=8192) now more manageable
#
# 2. PERFORMANCE TIPS:
#    - 8GB GPU:  batch_size=6-8   (vs 3-4 baseline)
#    - 12GB GPU: batch_size=12-16 (vs 6-8 baseline)
#    - 16GB GPU: batch_size=16-24 (vs 8-12 baseline)
#    - 24GB GPU: batch_size=32-48 (vs 16-24 baseline)
#
# 3. LEARNING RATE:
#    - Increased by ~1.4x (sqrt(2)) due to 2x larger batch size
#    - Original: 9.0e-05 â†’ CMX: 1.27e-04
#    - Monitor loss convergence and adjust if needed
#
# 4. QUALITY EXPECTATIONS:
#    - Expected SDR improvement: +0.1 to +0.3 dB for vocals
#    - Better harmonic preservation due to full frequency info retention
#    - Reduced artifacts from consistent multi-scale processing
#
# 5. TRAINING DYNAMICS:
#    - Similar convergence speed to baseline
#    - Larger batch = more stable gradients
#    - May converge slightly faster due to better gradient estimates
#
# 6. COMPARISON WITH BASELINE:
#    To compare with non-CMX version:
#    - Baseline: config_vocals_mdx23c.yaml (batch_size=6)
#    - CMX:      config_vocals_mdx23c_cmx.yaml (batch_size=12)
#    - Expected: Same quality, 2x throughput, 50% memory
#
# 7. TROUBLESHOOTING:
#    - OOM errors? Reduce batch_size by 25%
#    - Training unstable? Reduce lr to 1.0e-04
#    - Lower quality? Train longer (CMX may need more steps to converge)
#
# 8. RECOMMENDED WORKFLOW:
#    Step 1: Test with 10 epochs to verify memory savings
#    Step 2: Monitor GPU memory usage and loss curves
#    Step 3: Adjust batch_size to maximize GPU utilization
#    Step 4: Run full training with optimized settings
