audio:
  chunk_size: 352800
  dim_f: 1024
  dim_t: 256
  hop_length: 441
  n_fft: 2048
  num_channels: 2
  sample_rate: 44100
  min_mean_abs: 0.000

model:
  dim: 256
  depth: 12
  stereo: true
  num_stems: 1
  time_conformer_depth: 1
  freq_conformer_depth: 1
  num_bands: 60
  dim_head: 64
  heads: 8
  attn_dropout: 0
  ff_dropout: 0
  flash_attn: true
  dim_freqs_in: 1025
  sample_rate: 44100
  stft_n_fft: 2048
  stft_hop_length: 441
  stft_win_length: 2048
  stft_normalized: False
  freq_positional_encoding: absolute
  freq_positional_learned: false
  mask_estimator_depth: 2
  multi_stft_resolution_loss_weight: 1.0
  multi_stft_resolutions_window_sizes: !!python/tuple
  - 4096
  - 2048
  - 1024
  - 512
  - 256
  multi_stft_hop_size: 147
  multi_stft_normalized: False
  mlp_expansion_factor: 4
  ff_mult: 4
  conv_expansion_factor: 2
  conv_kernel_size: 31
  use_torch_checkpoint: false
  skip_connection: false
  sage_attention: false

training:
  batch_size: 1
  gradient_accumulation_steps: 1
  grad_clip: 0.0
  instruments:
  - vocals
  - other
  lr: 0.0001
  patience: 2
  reduce_factor: 0.95
  target_instrument: vocals
  num_epochs: 1000
  num_steps: 1000
  q: 0.95
  coarse_loss_clip: false
  ema_momentum: 0.999
  optimizer: adamw
  other_fix: false # it's needed for checking on multisong dataset if other is actually instrumental
  use_amp: true

inference:
  batch_size: 1
  dim_t: 801
  num_overlap: 2